<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Examples | Contrast Trees and Distribution Boosting in R</title>
  <meta name="description" content="This is a tutorial on the R package conTree." />
  <meta name="generator" content="bookdown 0.19 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Examples | Contrast Trees and Distribution Boosting in R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a tutorial on the R package conTree." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Examples | Contrast Trees and Distribution Boosting in R" />
  
  <meta name="twitter:description" content="This is a tutorial on the R package conTree." />
  

<meta name="author" content="Jerome H. Friedman" />


<meta name="date" content="2020-07-27" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="contree.html"/>
<link rel="next" href="acknowledgment.html"/>
<script src="libs/header-attrs-2.3/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Contrast Trees and Distribution Boosting in R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#contrast-trees"><i class="fa fa-check"></i><b>1.1</b> Contrast trees</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#contrast-boosting"><i class="fa fa-check"></i><b>1.2</b> Contrast boosting</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#distribution-boosting"><i class="fa fa-check"></i><b>1.3</b> Distribution boosting</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#two-sample-trees"><i class="fa fa-check"></i><b>1.4</b> Two-sample trees</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="contree.html"><a href="contree.html"><i class="fa fa-check"></i><b>2</b> <code>conTree</code></a>
<ul>
<li class="chapter" data-level="2.1" data-path="contree.html"><a href="contree.html#contrast"><i class="fa fa-check"></i><b>2.1</b> <code>contrast()</code></a></li>
<li class="chapter" data-level="2.2" data-path="contree.html"><a href="contree.html#nodesum"><i class="fa fa-check"></i><b>2.2</b> <code>nodesum()</code></a></li>
<li class="chapter" data-level="2.3" data-path="contree.html"><a href="contree.html#nodeplots"><i class="fa fa-check"></i><b>2.3</b> <code>nodeplots()</code></a></li>
<li class="chapter" data-level="2.4" data-path="contree.html"><a href="contree.html#treesum"><i class="fa fa-check"></i><b>2.4</b> <code>treesum()</code></a></li>
<li class="chapter" data-level="2.5" data-path="contree.html"><a href="contree.html#getnodes"><i class="fa fa-check"></i><b>2.5</b> <code>getnodes()</code></a></li>
<li class="chapter" data-level="2.6" data-path="contree.html"><a href="contree.html#lofcurve"><i class="fa fa-check"></i><b>2.6</b> <code>lofcurve()</code></a></li>
<li class="chapter" data-level="2.7" data-path="contree.html"><a href="contree.html#modtrast"><i class="fa fa-check"></i><b>2.7</b> <code>modtrast()</code></a></li>
<li class="chapter" data-level="2.8" data-path="contree.html"><a href="contree.html#xval"><i class="fa fa-check"></i><b>2.8</b> <code>xval()</code></a></li>
<li class="chapter" data-level="2.9" data-path="contree.html"><a href="contree.html#predtrast"><i class="fa fa-check"></i><b>2.9</b> <code>predtrast()</code></a></li>
<li class="chapter" data-level="2.10" data-path="contree.html"><a href="contree.html#ydist"><i class="fa fa-check"></i><b>2.10</b> <code>ydist()</code></a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="examples.html"><a href="examples.html"><i class="fa fa-check"></i><b>3</b> Examples</a>
<ul>
<li class="chapter" data-level="3.1" data-path="examples.html"><a href="examples.html#conditional-probability-estimation"><i class="fa fa-check"></i><b>3.1</b> Conditional probability estimation</a></li>
<li class="chapter" data-level="3.2" data-path="examples.html"><a href="examples.html#contrast-boosting-1"><i class="fa fa-check"></i><b>3.2</b> Contrast boosting</a></li>
<li class="chapter" data-level="3.3" data-path="examples.html"><a href="examples.html#conditional-distributions"><i class="fa fa-check"></i><b>3.3</b> Conditional distributions</a></li>
<li class="chapter" data-level="3.4" data-path="examples.html"><a href="examples.html#two-sample-contrast-trees"><i class="fa fa-check"></i><b>3.4</b> Two sample contrast trees</a></li>
<li class="chapter" data-level="3.5" data-path="examples.html"><a href="examples.html#session-information"><i class="fa fa-check"></i><b>3.5</b> Session Information</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="acknowledgment.html"><a href="acknowledgment.html"><i class="fa fa-check"></i><b>4</b> Acknowledgment</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Contrast Trees and Distribution Boosting in R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="examples" class="section level1" number="3">
<h1><span class="header-section-number">Chapter 3</span> Examples</h1>
<p>Here we provide examples of using different aspects of the <code>conTree</code>
package as applied to several data sets. These examples are intended to
illustrate some of the basic features, operation and control of the
package procedures. See the full documentation for more advanced
options. In this note lines beginning with " <span class="math inline">\(&gt;\)</span> " are to be interpreted
as R commands.</p>
<div id="conditional-probability-estimation" class="section level2" number="3.1">
<h2><span class="header-section-number">3.1</span> Conditional probability estimation</h2>
<p>Here we present R code and resulting output using <code>conTree</code> for the
analysis presented in Section 6.2 of <span class="citation">Friedman (<a href="#ref-FriedmanPNAS" role="doc-biblioref">2020</a>)</span> using the census
income data. This sample, taken from 1994 US census data, consists of
observations from 48842 people divided into a training set of 32561 and
an independent test set of 16281. The binary outcome variable
<span class="math inline">\(y\in\{0,1\}\)</span> indicates whether or not a person’s income is greater than
$50000 per year. There are 14 predictor variables <span class="math inline">\(\mathbf{x}\)</span>
consisting of various demographic and financial properties associated
with each person. The goal is to contrast <span class="math inline">\(y\)</span> with estimates of
<span class="math inline">\(\Pr(y=1\,|\,\mathbf{x})\)</span> obtained by several machine learning methods:
gradient boosting on logistic scale using maximum likelihood (GBL),
random forest (RF), and gradient boosting on the probability scale (GBP)
using least–squares.</p>
<p>First load <code>conTree</code> package</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="examples.html#cb11-1" aria-hidden="true"></a><span class="kw">library</span>(<span class="st">`</span><span class="dt">conTree</span><span class="st">`</span>)</span></code></pre></div>
<p>The next step is to load and attach the data with the R command</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="examples.html#cb12-1" aria-hidden="true"></a><span class="kw">data</span>(census, <span class="dt">package =</span> <span class="st">&quot;conTree&quot;</span>)</span>
<span id="cb12-2"><a href="examples.html#cb12-2" aria-hidden="true"></a><span class="kw">attach</span>(census)</span></code></pre></div>
<p>This creates the following numeric vectors and data frames :</p>
<table>
<thead>
<tr class="header">
<th align="left">Variable</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><code>x</code></td>
<td align="left">training data predictor variables (data frame)</td>
</tr>
<tr class="even">
<td align="left"><code>y</code></td>
<td align="left">training data outcome (high salary indicator)</td>
</tr>
<tr class="odd">
<td align="left"><code>xt</code></td>
<td align="left">test data predictor variables (data frame)</td>
</tr>
<tr class="even">
<td align="left"><code>yt</code></td>
<td align="left">test data outcome</td>
</tr>
<tr class="odd">
<td align="left"><code>gbl</code></td>
<td align="left">training data GBL estimates</td>
</tr>
<tr class="even">
<td align="left"><code>gblt</code></td>
<td align="left">test data GBL estimates</td>
</tr>
<tr class="odd">
<td align="left"><code>rf</code></td>
<td align="left">training data RF estimates</td>
</tr>
<tr class="even">
<td align="left"><code>rft</code></td>
<td align="left">test data RF estimates</td>
</tr>
<tr class="odd">
<td align="left"><code>gbp</code></td>
<td align="left">training data GBP estimates</td>
</tr>
<tr class="even">
<td align="left"><code>gbpt</code></td>
<td align="left">test data GBP estimates</td>
</tr>
</tbody>
</table>
<p>Note that all of the <span class="math inline">\(\Pr(y=1\,|\,\mathbf{x})\)</span> estimates were obtained
using the training data (<span class="math inline">\(\mathtt{x}\)</span>,<code>y</code>) only.</p>
<p>Partition <em>test</em> data into two parts</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="examples.html#cb13-1" aria-hidden="true"></a>dx =<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">10000</span></span>
<span id="cb13-2"><a href="examples.html#cb13-2" aria-hidden="true"></a>dxt =<span class="st"> </span><span class="dv">10001</span><span class="op">:</span><span class="dv">16281</span></span></code></pre></div>
<p>Contrast <span class="math inline">\(y\)</span> and GBL estimates using first part</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="examples.html#cb14-1" aria-hidden="true"></a>tree =<span class="st"> </span><span class="kw">contrast</span>(xt[dx,], yt[dx], gblt[dx], <span class="dt">type =</span> <span class="st">&#39;prob&#39;</span>)</span></code></pre></div>
<p>Validate on second part which produces the command line output:</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="examples.html#cb15-1" aria-hidden="true"></a><span class="kw">nodesum</span>(tree, xt[dxt,], yt[dxt], gblt[dxt])</span></code></pre></div>
<pre><code>## $nodes
##  [1]  7  6 29 11 28 16  9 23 13 22
## 
## $cri
##  [1] 0.11499156 0.10586120 0.09498529 0.08688654 0.06500579 0.04966835
##  [7] 0.04906626 0.04751317 0.03595857 0.02631912
## 
## $wt
##  [1]  561  796  441  411  423  361 1612  431  795  450
## 
## $avecri
## [1] 0.06556377</code></pre>
<p>The first component <code>$nodes</code> lists the tree node identifiers for each
terminal node in order of region discrepancy. The second <code>$cri</code> gives
the actual discrepancy of each corresponding region. The third component
<code>$wt</code> shows the corresponding number of observations in each region. The
quantity <code>$avecri</code> is the observation weighted average discrepancy over
all regions.</p>
<p>The function <code>nodeplots()</code> produces the graphical tree summary shown
in Figure <a href="examples.html#fig:fig1">3.1</a>.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="examples.html#cb17-1" aria-hidden="true"></a><span class="kw">nodeplots</span>(tree, xt[dxt,], yt[dxt], gblt[dxt])</span></code></pre></div>
<div class="figure"><span id="fig:fig1"></span>
<img src="conTree_tutorial_files/figure-html/fig1-1.png" alt="Graphical tree summary." width="672" />
<p class="caption">
Figure 3.1: Graphical tree summary.
</p>
</div>
<p>The blue bars in the upper barplot represent the empirical <span class="math inline">\(\Pr(y=1)\)</span>
in each region whereas the red ones show the mean of the GBL
predictions in the corresponding regions. One sees dramatic over
estimation of the small probabilities.</p>
<p>Region boundaries for selected nodes 7 and 29 can be obtained by:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="examples.html#cb18-1" aria-hidden="true"></a><span class="kw">treesum</span>(tree, <span class="kw">c</span>(<span class="dv">7</span>, <span class="dv">29</span>))</span></code></pre></div>
<pre><code>## node 7  var     dir    split 
##       cat 6      -     2 3 5 
##       cat 7      -     1 2 7 8 9 10 11 
## node 29  var     dir    split 
##       cat 6      +     2 3 5 
##       cat 8      -     1 4 6 
##           5      -      12 
##           1      +      28</code></pre>
<p>Interpretation of this output is described in Section <a href="contree.html#treesum">2.4</a>.</p>
<p>The command</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="examples.html#cb20-1" aria-hidden="true"></a>nx =<span class="st"> </span><span class="kw">getnodes</span>(tree, xt)</span></code></pre></div>
<p>obtains the terminal node identity for each test observation for the
tree represented in Figure <a href="examples.html#fig:fig1">3.1</a>. The commands</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="examples.html#cb21-1" aria-hidden="true"></a><span class="kw">plot</span>(gblt[nx <span class="op">==</span><span class="st"> </span><span class="dv">7</span>], gbpt[nx <span class="op">==</span><span class="st"> </span><span class="dv">7</span>], <span class="dt">pch =</span> <span class="st">&#39;.&#39;</span>, <span class="dt">xlab =</span> <span class="st">&#39;GBTL&#39;</span>, <span class="dt">ylab =</span> <span class="st">&#39;GBPT&#39;</span>)</span>
<span id="cb21-2"><a href="examples.html#cb21-2" aria-hidden="true"></a><span class="kw">lines</span>(<span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">col =</span> <span class="st">&#39;red&#39;</span>)</span></code></pre></div>
<div class="figure"><span id="fig:fig2"></span>
<img src="conTree_tutorial_files/figure-html/fig2-1.png" alt="GBL vrsus GBP probability estimates for test data observations in region 7." width="672" />
<p class="caption">
Figure 3.2: GBL vrsus GBP probability estimates for test data observations in region 7.
</p>
</div>
<p>plot the test data GBP predictions against those of GBL for the
highest discrepancy region 7. The red line represents equality. The
result shown in Figure <a href="examples.html#fig:fig2">3.2</a> indicates that the gradient
boosting probabilities based on log-odds estimates in this region are
considerably larger but proportional to those obtained by direct
probability estimation using least-squares. One also sees the effect
of truncating the out of range estimates produced by the latter. In
spite of this the GBP probability estimates are seen in Figure
<a href="examples.html#fig:fig3">3.3</a> (below) to be from three to four times more accurate
than those of GBL</p>
<p>We next plot the lack-of-fit contrast curve for GBL using a 50 terminal
node contrast tree</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="examples.html#cb22-1" aria-hidden="true"></a>tree =<span class="st"> </span><span class="kw">contrast</span>(xt[dx,], yt[dx], gblt, <span class="dt">type =</span> <span class="st">&#39;prob&#39;</span>, <span class="dt">tree.size =</span> <span class="dv">50</span>)</span>
<span id="cb22-2"><a href="examples.html#cb22-2" aria-hidden="true"></a><span class="kw">lofcurve</span>(tree, xt[dxt,], yt[dxt], gblt[dxt], <span class="dt">doplot =</span> <span class="ot">TRUE</span>)</span></code></pre></div>
<p>Next we add the corresponding curve for RF predicted probabilities</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="examples.html#cb23-1" aria-hidden="true"></a>tree =<span class="st"> </span><span class="kw">contrast</span>(xt[dx, ], yt[dx], rft[dx, <span class="dv">2</span>], <span class="dt">type =</span> <span class="st">&#39;prob&#39;</span>, <span class="dt">tree.size =</span> <span class="dv">50</span>)</span>
<span id="cb23-2"><a href="examples.html#cb23-2" aria-hidden="true"></a>u =<span class="st"> </span><span class="kw">lofcurve</span>(tree, xt[dxt, ], yt[dxt], rft[dxt, <span class="dv">2</span>], <span class="dt">doplot =</span> <span class="ot">FALSE</span>)</span>
<span id="cb23-3"><a href="examples.html#cb23-3" aria-hidden="true"></a><span class="kw">lines</span>(u<span class="op">$</span>x, u<span class="op">$</span>y, <span class="dt">col =</span> <span class="st">&#39;blue&#39;</span>)</span></code></pre></div>
<p>And finally that for the GBP estimates</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="examples.html#cb24-1" aria-hidden="true"></a>tree =<span class="st"> </span><span class="kw">contrast</span>(xt[dx, ], yt[dx], gbpt[dx], <span class="dt">type =</span> <span class="st">&#39;prob&#39;</span>, <span class="dt">tree.size =</span> <span class="dv">50</span>)</span>
<span id="cb24-2"><a href="examples.html#cb24-2" aria-hidden="true"></a>u =<span class="st"> </span><span class="kw">lofcurve</span>(tree, xt[dxt, ], yt[dxt], gbpt[dxt], <span class="dt">doplot =</span> <span class="ot">FALSE</span>)</span>
<span id="cb24-3"><a href="examples.html#cb24-3" aria-hidden="true"></a><span class="kw">lines</span>(u<span class="op">$</span>x, u<span class="op">$</span>y, <span class="dt">col =</span> <span class="st">&#39;red&#39;</span>)</span></code></pre></div>
<div class="figure"><span id="fig:fig3"></span>
<img src="conTree_tutorial_files/figure-html/fig3-1.png" alt="Lack-of-fit contrast curves for GBL (black), RF (blue), and GBP (red) for census income data." width="672" />
<p class="caption">
Figure 3.3: Lack-of-fit contrast curves for GBL (black), RF (blue), and GBP (red) for census income data.
</p>
</div>
<p>The result is shown in Figure <a href="examples.html#fig:fig3">3.3</a>.</p>
</div>
<div id="contrast-boosting-1" class="section level2" number="3.2">
<h2><span class="header-section-number">3.2</span> Contrast boosting</h2>
<p>Here we illustrate the use of contrast boosting to improve prediction
performance of GBL. The model is built using the <em>training</em> data with
the command</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="examples.html#cb25-1" aria-hidden="true"></a>modgbl =<span class="st"> </span><span class="kw">modtrast</span>(x, y, gbl, <span class="dt">type =</span> <span class="st">&#39;prob&#39;</span>, <span class="dt">niter =</span> <span class="dv">200</span>)</span></code></pre></div>
<pre><code>## .............................</code></pre>
<p>We can plot tree average discrepancy versus iteration number on the
training data and super impose a corresponding plot based on the test
data set with the commands:</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="examples.html#cb27-1" aria-hidden="true"></a><span class="kw">xval</span>(modgbl, x, y, gbl, <span class="dt">col =</span> <span class="st">&#39;red&#39;</span>)</span></code></pre></div>
<pre><code>## .....................</code></pre>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="examples.html#cb29-1" aria-hidden="true"></a><span class="kw">xval</span>(modgbl, xt, yt, gblt, <span class="dt">col =</span> <span class="st">&#39;green&#39;</span>, <span class="dt">doplot =</span> <span class="st">&#39;next&#39;</span>)</span></code></pre></div>
<div class="figure"><span id="fig:fig4"></span>
<img src="conTree_tutorial_files/figure-html/fig4-1.png" alt="`xval(x, y, gbl, modgbl, col = 'red')`" width="672" />
<p class="caption">
Figure 3.4: <code>xval(x, y, gbl, modgbl, col = 'red')</code>
</p>
</div>
<pre><code>## .....................</code></pre>
<p>The result is shown in Figure <a href="examples.html#fig:fig4">3.4</a>. Note that results at
every tenth iteration are shown.</p>
<p>Model predictions on the test data set can be obtained with the command</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="examples.html#cb31-1" aria-hidden="true"></a>hblt =<span class="st"> </span><span class="kw">predtrast</span>(modgbl, xt, gblt)</span></code></pre></div>
<p>These can be contrasted with the <span class="math inline">\(y\)</span> - values using the first test set
sample</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="examples.html#cb32-1" aria-hidden="true"></a>tree =<span class="st"> </span><span class="kw">contrast</span>(xt[dx, ], yt[dx], hblt[dx], <span class="dt">type =</span> <span class="st">&#39;prob&#39;</span>)</span></code></pre></div>
<p>and summarized on the other test sample with the command</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="examples.html#cb33-1" aria-hidden="true"></a><span class="kw">nodeplots</span>(tree, xt[dxt, ], yt[dxt], hblt[dxt])</span></code></pre></div>
<div class="figure"><span id="fig:fig5"></span>
<img src="conTree_tutorial_files/figure-html/fig5-1.png" alt="`nodeplots(tree, xt[dxt, ], yt[dxt], hblt[dxt])`" width="672" />
<p class="caption">
Figure 3.5: <code>nodeplots(tree, xt[dxt, ], yt[dxt], hblt[dxt])</code>
</p>
</div>
<p>producing Figure <a href="examples.html#fig:fig5">3.5</a>.</p>
<p>Comparing with Figure <a href="examples.html#fig:fig5">3.5</a> one sees that contrast boosting
the GBL model appears to have substantially reduced the shrinking of
the of the extreme probabilities estimates thereby improving its
conditional probability estimation accuracy.</p>
<p>One can boost the RF and GBP models in the same way using the commands
analogous with those used here for the GBL model. Additionally,
lack-of-fit contrast curves can be produced and plotted for all three
boosted models using modifications of the corresponding commands above.
The results are shown in Fig. 4 of Friedman (2020).</p>
</div>
<div id="conditional-distributions" class="section level2" number="3.3">
<h2><span class="header-section-number">3.3</span> Conditional distributions</h2>
<p>We illustrate contrasting and estimating conditional distributions
<span class="math inline">\(p_{y}(y\,|\,\mathbf{x})\)</span> using the demographics data set described in
Table 14.1 of Hastie, Tibshirani and Friedman (2008). Here we attempt to
estimate a persons age as a function of the other 13 demographic
variables. For this data set age value is reported as being in one of
seven intervals
<span class="math display">\[age\in\text{\{13-17, 18-24, 25-34, 35-44, 45-54, 55-64,}\geq\text{ 65\}.}\]</span>
As input to the algorithm each persons age is randomly generated
uniformly within its corresponding reported interval. For the last
interval an exponential distribution was used with mean corresponding to
life expectancy after reaching age 65.</p>
<p>We first load the data and attach it.</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="examples.html#cb34-1" aria-hidden="true"></a><span class="kw">data</span>(age_data, <span class="dt">package =</span> <span class="st">&quot;conTree&quot;</span>)</span>
<span id="cb34-2"><a href="examples.html#cb34-2" aria-hidden="true"></a><span class="kw">attach</span>(age_data)</span></code></pre></div>
<p>This loads the following numeric vectors and data frames:</p>
<table>
<thead>
<tr class="header">
<th align="left">Variable</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><code>xage</code></td>
<td align="left">outcome variable (age)</td>
</tr>
<tr class="even">
<td align="left"><code>yage</code></td>
<td align="left">predictor variables (other demographics - data frame)</td>
</tr>
<tr class="odd">
<td align="left"><code>gbage</code></td>
<td align="left">gradient boosting model for median (yage <span class="math inline">\(\vert\)</span> xage)</td>
</tr>
</tbody>
</table>
<p>The command</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="examples.html#cb35-1" aria-hidden="true"></a><span class="kw">hist</span>(yage)</span></code></pre></div>
<div class="figure"><span id="fig:fig6"></span>
<img src="conTree_tutorial_files/figure-html/fig6-1.png" alt="`hist(age)`" width="672" />
<p class="caption">
Figure 3.6: <code>hist(age)</code>
</p>
</div>
<p>produces the marginal age distribution as seen in Figure <a href="examples.html#fig:fig6">3.6</a>.</p>
<p>This data is divided into training and test data subsets</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="examples.html#cb36-1" aria-hidden="true"></a>dl =<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">5000</span> <span class="co"># training data`</span></span>
<span id="cb36-2"><a href="examples.html#cb36-2" aria-hidden="true"></a>dt =<span class="st"> </span><span class="dv">5001</span><span class="op">:</span><span class="dv">8856</span> <span class="co"># test data`</span></span></code></pre></div>
<p>We next contrast the distribution of <span class="math inline">\(y\,|\,\mathbf{x}\)</span> with that of
the no information hypothesis <span class="math inline">\(p_{y}(y\,|\,\mathbf{x})=\hat{p}_{y}(y)\)</span>
where <span class="math inline">\(\hat {p}_{y}(y)\)</span> is the empirical marginal <span class="math inline">\(y\)</span> - distribution
independent of <span class="math inline">\(\mathbf{x}\)</span> as shown in Figure <a href="examples.html#fig:fig6">3.6</a>. First
create independent marginal distribution of <span class="math inline">\(y\)</span> as the contrasting
distribution</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="examples.html#cb37-1" aria-hidden="true"></a><span class="kw">set.seed</span>(<span class="dv">5</span>)</span>
<span id="cb37-2"><a href="examples.html#cb37-2" aria-hidden="true"></a>zage =<span class="st"> </span>yage[<span class="kw">sample.int</span>(<span class="kw">length</span>(yage))]</span></code></pre></div>
<p>Then contrast with <span class="math inline">\(y\)</span> - distribution</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="examples.html#cb38-1" aria-hidden="true"></a>treezage =<span class="st"> </span><span class="kw">contrast</span>(xage[dt, ], yage[dt], zage[dt], <span class="dt">tree.size =</span> <span class="dv">9</span>, <span class="dt">min.node =</span> <span class="dv">200</span>)</span></code></pre></div>
<p>with tree test set command line summary produced by</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb39-1"><a href="examples.html#cb39-1" aria-hidden="true"></a><span class="kw">nodesum</span>(treezage, xage[dt, ], yage[dt], zage[dt])</span></code></pre></div>
<pre><code>## $nodes
## [1]  7  6 17 26 11 27  4 20
## 
## $cri
## [1] 0.9600578 0.7737634 0.5972841 0.5755558 0.5276239 0.4381393 0.1946532
## [8] 0.1542807
## 
## $wt
## [1]  320  597  268  367  304  294 1477  229
## 
## $avecri
## [1] 0.4544866</code></pre>
<p>and graphical summary produced by</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb41-1"><a href="examples.html#cb41-1" aria-hidden="true"></a><span class="kw">nodeplots</span>(treezage, xage[dt, ], yage[dt], zage[dt])</span></code></pre></div>
<div class="figure"><span id="fig:fig7"></span>
<img src="conTree_tutorial_files/figure-html/fig7-1.png" alt="`nodeplots(treezage, xage[dt, ], yage[dt], zage[dt])`" width="672" />
<p class="caption">
Figure 3.7: <code>nodeplots(treezage, xage[dt, ], yage[dt], zage[dt])</code>
</p>
</div>
<p>shown in Figure @ref{fig:fig7).</p>
<p>Each frame in Figure @ref{fig:fig7) shows a QQ-plot of the
distribution of age <span class="math inline">\(y\)</span> versus that of its <span class="math inline">\(\mathbf{x}\)</span>
independent marginal counterpart (Figure <a href="examples.html#fig:fig6">3.6</a>) in each of
eight regions returned by contrast. To the extent the two
distributions are similar the points would lie close to the diagonal
(red) line. Here they are see to be very different indicating that
<span class="math inline">\(p_{y}(y\,|\,\mathbf{x})\)</span> is highly dependent on <span class="math inline">\(\mathbf{x}\)</span>.</p>
<p>We next construct <span class="math inline">\(y_{B}\sim p_{B}(y\,|\,\mathbf{x)}\)</span>, the residual
bootstrap approximation to <span class="math inline">\(p_{y}(y\,|\,\mathbf{x)}\)</span> using gradient
boosting median estimates for its location. This assumes that
<span class="math inline">\(p_{y}(y\,|\,\mathbf{x)}\)</span> is homoskedastic with varying location</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="examples.html#cb42-1" aria-hidden="true"></a>res =<span class="st"> </span>yage <span class="op">-</span><span class="st"> </span>gbage</span></code></pre></div>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="examples.html#cb43-1" aria-hidden="true"></a>rbage =<span class="st"> </span>gbage <span class="op">+</span><span class="st"> </span>res[<span class="kw">sample.int</span>(<span class="kw">length</span>(res))]</span></code></pre></div>
<p>and contrast <span class="math inline">\(y\)</span> with <span class="math inline">\(y_{B}\)</span> on the test data</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="examples.html#cb44-1" aria-hidden="true"></a>treerbage =<span class="st"> </span><span class="kw">contrast</span>(xage[dt, ], yage[dt], rbage[dt], <span class="dt">tree.size =</span> <span class="dv">9</span>, <span class="dt">min.node =</span> <span class="dv">200</span>)</span></code></pre></div>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="examples.html#cb45-1" aria-hidden="true"></a><span class="kw">nodeplots</span>(treerbage, xage[dt, ], yage[dt], rbage[dt])</span></code></pre></div>
<div class="figure"><span id="fig:fig8"></span>
<img src="conTree_tutorial_files/figure-html/fig8-1.png" alt="`nodeplots(treerbage, xage[dt, ], yage[dt], rbage[dt])`" width="672" />
<p class="caption">
Figure 3.8: <code>nodeplots(treerbage, xage[dt, ], yage[dt], rbage[dt])</code>
</p>
</div>
<p>The result is shown in Figure @ref{fig:fig8).</p>
<p>Although not perfect, the residual bootstrap <span class="math inline">\(p_{B}(y\,|\,\mathbf{x)}\)</span>
is seen to provide a much closer approximation to
<span class="math inline">\(p_{y}(y\,|\,\mathbf{x)}\)</span> than the global marginal <span class="math inline">\(p_{y}(y)\)</span>. This
indicates that its location has a strong dependence on <span class="math inline">\(\mathbf{x}\)</span> as
captured by the gradient boosting conditional median estimate.</p>
<p>Next we attempt to further improve the estimate of
<span class="math inline">\(p_{y}(y\,|\,\mathbf{x)}\)</span> by applying distribution boosting to the
training data starting with the residual boostrap approximation.</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="examples.html#cb46-1" aria-hidden="true"></a>mdlrb =<span class="st"> </span><span class="kw">modtrast</span>(xage[dl, ], yage[dl], rbage[dl], <span class="dt">min.node =</span> <span class="dv">200</span>)</span></code></pre></div>
<pre><code>## ...................</code></pre>
<p>The commands</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="examples.html#cb48-1" aria-hidden="true"></a><span class="kw">xval</span>(mdlrb, xage[dl, ], yage[dl], rbage[dl], <span class="dt">col =</span> <span class="st">&#39;red&#39;</span>)</span></code></pre></div>
<pre><code>## ...........</code></pre>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="examples.html#cb50-1" aria-hidden="true"></a><span class="kw">xval</span>(mdlrb, xage[dt, ], yage[dt], rbage[dt], <span class="dt">col =</span> <span class="st">&#39;green&#39;</span>, <span class="dt">doplot =</span> <span class="st">&#39;next&#39;</span>)</span></code></pre></div>
<div class="figure"><span id="fig:fig9"></span>
<img src="conTree_tutorial_files/figure-html/fig9-1.png" alt="`xval(mdlrb, xage[dl, ], yage[dl], rbage[dl], col = 'red')`" width="672" />
<p class="caption">
Figure 3.9: <code>xval(mdlrb, xage[dl, ], yage[dl], rbage[dl], col = 'red')</code>
</p>
</div>
<pre><code>## ...........</code></pre>
<p>produce a plot of training (red) and test (green) data average tree
discrepancy as a function of iteration number (every 10th iteration) as
shown is Figure <a href="examples.html#fig:fig9">3.9</a>.</p>
<p>The command</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="examples.html#cb52-1" aria-hidden="true"></a>hrbage =<span class="st"> </span><span class="kw">predtrast</span>(mdlrb, xage[dt, ], rbage[dt])</span></code></pre></div>
<p>transforms the test data residual bootstrap distribution
<span class="math inline">\(y_{B}\sim p_{B}(y\,|\,\mathbf{x)}\)</span> to the <span class="math inline">\(y\)</span> distribution estimate
<span class="math inline">\(\hat{y}\sim\hat{p}_{\hat{y}}(y\,|\,\mathbf{x)}\)</span>.</p>
<p>The commands</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="examples.html#cb53-1" aria-hidden="true"></a>treehrbage =<span class="st"> </span><span class="kw">contrast</span>(xage[dt, ], yage[dt], hrbage, <span class="dt">tree.size =</span> <span class="dv">9</span>, <span class="dt">min.node =</span> <span class="dv">200</span>)</span>
<span id="cb53-2"><a href="examples.html#cb53-2" aria-hidden="true"></a><span class="kw">nodeplots</span>(treerbage, xage[dt, ], yage[dt], hrbage)</span></code></pre></div>
<div class="figure"><span id="fig:fig10"></span>
<img src="conTree_tutorial_files/figure-html/fig10-1.png" alt="`nodeplots(treerbage, xage[dt, ], yage[dt], hrbage)`" width="672" />
<p class="caption">
Figure 3.10: <code>nodeplots(treerbage, xage[dt, ], yage[dt], hrbage)</code>
</p>
</div>
<p>contrast the transformed distribution <span class="math inline">\(\hat{y}\)</span> with that of <span class="math inline">\(y\)</span> on the
test data. Results are shown in Figure <a href="examples.html#fig:fig10">3.10</a>.</p>
<p>As seen the results are not perfect but somewhat better than that for
the residual bootstrap distribution shown in Figure <a href="examples.html#fig:fig8">3.8</a>.
This is verified by the corresponding lack-of-fit contrast curves
shown in Figure <a href="examples.html#fig:fig11">3.11</a> as produced by the commands</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="examples.html#cb54-1" aria-hidden="true"></a><span class="kw">lofcurve</span>(treezage, xage[dt, ], yage[dt], zage[dt])</span>
<span id="cb54-2"><a href="examples.html#cb54-2" aria-hidden="true"></a>u =<span class="st"> </span><span class="kw">lofcurve</span>(treerbage, xage[dt, ], yage[dt], rbage[dt], <span class="dt">doplot =</span> <span class="ot">FALSE</span>)</span>
<span id="cb54-3"><a href="examples.html#cb54-3" aria-hidden="true"></a><span class="kw">lines</span>(u<span class="op">$</span>x, u<span class="op">$</span>y, <span class="dt">col =</span> <span class="st">&#39;blue&#39;</span>)</span>
<span id="cb54-4"><a href="examples.html#cb54-4" aria-hidden="true"></a>u =<span class="st"> </span><span class="kw">lofcurve</span>(treehrbage, xage[dt, ], yage[dt], hrbage, <span class="dt">doplot =</span> <span class="ot">FALSE</span>)</span>
<span id="cb54-5"><a href="examples.html#cb54-5" aria-hidden="true"></a><span class="kw">lines</span>(u<span class="op">$</span>x, u<span class="op">$</span>y, <span class="dt">col =</span> <span class="st">&#39;red&#39;</span>)</span></code></pre></div>
<div class="figure"><span id="fig:fig11"></span>
<img src="conTree_tutorial_files/figure-html/fig11-1.png" alt="Lack-of-fit contrast curves" width="672" />
<p class="caption">
Figure 3.11: Lack-of-fit contrast curves
</p>
</div>
<p>The lack-of-fit contrast curves are plotted for the three estimates of
<span class="math inline">\(p_{y}(y\,|\,\mathbf{x})\)</span>: global marginal <span class="math inline">\(p_{y}(y)\)</span> (black), residual
bootstrap <span class="math inline">\(p_{B}(y\,|\,\mathbf{x)}\)</span> (blue) and distribution boosting
estimate <span class="math inline">\(\hat{p}_{y}(y\,|\,\mathbf{x})\)</span> (red). Distribution boosting is
seen to improve the accuracy of the conditional distribution estimate by
roughly a factor of two.</p>
<p>Finally we estimate <span class="math inline">\(p_{y}(y\,|\,\mathbf{x})\)</span> at <span class="math inline">\(\mathbf{x}\)</span> - values
for nine selected observations</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb55-1"><a href="examples.html#cb55-1" aria-hidden="true"></a>obs =<span class="st"> </span><span class="kw">c</span>(<span class="dv">8843</span>, <span class="dv">5716</span>, <span class="dv">7831</span>, <span class="dv">6505</span>, <span class="dv">4949</span>, <span class="dv">7555</span>, <span class="dv">3202</span>, <span class="dv">6048</span>, <span class="dv">7134</span>)</span></code></pre></div>
<p>at 500 quantile values</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="examples.html#cb56-1" aria-hidden="true"></a>p =<span class="st"> </span>((<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>) <span class="fl">-0.5</span>) <span class="op">/</span><span class="st"> </span><span class="dv">500</span></span>
<span id="cb56-2"><a href="examples.html#cb56-2" aria-hidden="true"></a>qres =<span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">quantile</span>(res, p))</span></code></pre></div>
<p>and plot their CDFs with</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb57-1"><a href="examples.html#cb57-1" aria-hidden="true"></a>opar &lt;-<span class="st"> </span><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">3</span>))</span>
<span id="cb57-2"><a href="examples.html#cb57-2" aria-hidden="true"></a><span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">9</span>) {</span>
<span id="cb57-3"><a href="examples.html#cb57-3" aria-hidden="true"></a>    <span class="kw">plot</span>(<span class="kw">ydist</span>(mdlrb, xage[obs[k], ], gbage[obs[k]] <span class="op">+</span><span class="st"> </span>qres), p, <span class="dt">type =</span> <span class="st">&#39;l&#39;</span>, <span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">13</span>, <span class="dv">100</span>),</span>
<span id="cb57-4"><a href="examples.html#cb57-4" aria-hidden="true"></a>         <span class="dt">xlab =</span> <span class="st">&#39;Age&#39;</span>, <span class="dt">main =</span> <span class="kw">paste</span>(<span class="st">&#39;Observation&#39;</span>, <span class="kw">as.character</span>(obs[k])))</span>
<span id="cb57-5"><a href="examples.html#cb57-5" aria-hidden="true"></a>    <span class="kw">points</span>(yage[obs[k]], <span class="dv">0</span>, <span class="dt">col =</span> <span class="st">&#39;red&#39;</span>)</span>
<span id="cb57-6"><a href="examples.html#cb57-6" aria-hidden="true"></a>    <span class="kw">title</span>(<span class="kw">paste</span>(<span class="st">&#39;Observation&#39;</span>, obs[k]))</span>
<span id="cb57-7"><a href="examples.html#cb57-7" aria-hidden="true"></a>}</span></code></pre></div>
<div class="figure"><span id="fig:fig12"></span>
<img src="conTree_tutorial_files/figure-html/fig12-1.png" alt="CDF estimates for nine $\mathbf{x}$-values." width="672" />
<p class="caption">
Figure 3.12: CDF estimates for nine <span class="math inline">\(\mathbf{x}\)</span>-values.
</p>
</div>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb58-1"><a href="examples.html#cb58-1" aria-hidden="true"></a><span class="kw">par</span>(opar)</span></code></pre></div>
<p>as shown in Figure @ref{fig:fig12). The red points shown at the bottom of each plot
display the actual realized <span class="math inline">\(y\)</span> - value (age) for that observation.
Prediction intervals for each observation can be read directly from its
corresponding CDF display.</p>
<p>Probability densities for these observations can be visualized with the
commands</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="examples.html#cb59-1" aria-hidden="true"></a>opar &lt;-<span class="st"> </span><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">3</span>))</span>
<span id="cb59-2"><a href="examples.html#cb59-2" aria-hidden="true"></a><span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">9</span>) {</span>
<span id="cb59-3"><a href="examples.html#cb59-3" aria-hidden="true"></a>    <span class="kw">hist</span>(<span class="kw">ydist</span>(mdlrb, xage[obs[k], ], gbage[obs[k]] <span class="op">+</span><span class="st"> </span>qres), <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">13</span>, <span class="dv">100</span>), <span class="dt">nclass =</span> <span class="dv">10</span>,</span>
<span id="cb59-4"><a href="examples.html#cb59-4" aria-hidden="true"></a>         <span class="dt">xlab =</span> <span class="st">&#39;Age&#39;</span>, <span class="dt">main =</span> <span class="kw">paste</span>(<span class="st">&#39;Observation&#39;</span>, <span class="kw">as.character</span>(obs[k])))</span>
<span id="cb59-5"><a href="examples.html#cb59-5" aria-hidden="true"></a>    <span class="kw">points</span>(yage[obs[k]], <span class="dv">0</span>, <span class="dt">col =</span> <span class="st">&#39;red&#39;</span>)</span>
<span id="cb59-6"><a href="examples.html#cb59-6" aria-hidden="true"></a>    <span class="kw">title</span>(<span class="kw">paste</span>(<span class="st">&#39;Observation&#39;</span>, obs[k]))</span>
<span id="cb59-7"><a href="examples.html#cb59-7" aria-hidden="true"></a>}</span></code></pre></div>
<div class="figure"><span id="fig:fig13"></span>
<img src="conTree_tutorial_files/figure-html/fig13-1.png" alt="Corresponding probability densities for the nine observations." width="672" />
<p class="caption">
Figure 3.13: Corresponding probability densities for the nine observations.
</p>
</div>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="examples.html#cb60-1" aria-hidden="true"></a><span class="kw">par</span>(opar)</span></code></pre></div>
<p>as seen in Figure <a href="examples.html#fig:fig13">3.13</a>.</p>
<p>Considerable heteroskedasticity and skewness in the estimated
conditional distributions are evident.</p>
</div>
<div id="two-sample-contrast-trees" class="section level2" number="3.4">
<h2><span class="header-section-number">3.4</span> Two sample contrast trees</h2>
<p>The use of two sample contrast trees is illustrated on the air quality
data set also from the Irvine Machine Learning Data Repository. The data
set consists of hourly averaged measurements from an array of 5 metal
oxide chemical sensors embedded in an air quality chemical multisensor
device. The outcome variable <span class="math inline">\(y\)</span> is  the corresponding true hourly
averaged concentration CO taken from a reference analyzer. The input
variables <span class="math inline">\(\mathbf{x}\)</span> are taken to be the corresponding hourly averaged
measurements of the other 13 quantities as described at the download web
site. The goal here is to contrast the conditional distribution of
<span class="math inline">\(y|\,\mathbf{x}\)</span> for data taken in the first six months (January –
June) to that of the last six months (July – December).</p>
<p>The first step is to load the data</p>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="examples.html#cb61-1" aria-hidden="true"></a><span class="kw">data</span>(air_quality)</span>
<span id="cb61-2"><a href="examples.html#cb61-2" aria-hidden="true"></a><span class="kw">attach</span>(air_quality)</span></code></pre></div>
<p>This loads in three vectors and a data frame</p>
<table>
<thead>
<tr class="header">
<th align="left">Variable</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><code>yco</code></td>
<td align="left">outcome variable (CO concentration)</td>
</tr>
<tr class="even">
<td align="left"><code>xco</code></td>
<td align="left">predictor variables (data frame)</td>
</tr>
<tr class="odd">
<td align="left"><code>zco</code></td>
<td align="left">sample membership indicator</td>
</tr>
<tr class="even">
<td align="left"><code>pr2</code></td>
<td align="left">probability propensity score.</td>
</tr>
</tbody>
</table>
<p>The first quantity <code>yco</code> is the outcome variable <span class="math inline">\(y\)</span>, <code>xco</code> is a data
frame containing the 13 predictor variables <span class="math inline">\(\mathbf{x}\)</span> for each
observation and <code>zco</code> indicates sample membership: <code>zco &lt; 0</code>
<span class="math inline">\(\Rightarrow\)</span> first six months, <code>zco &gt; 0</code> <span class="math inline">\(\Rightarrow\)</span> last six months.
The vector <code>pr2</code> contains gradient boosting model predictions for
<span class="math inline">\(\Pr(z&gt;0\,|\,\mathbf{x})\)</span>; that is the predicted probability of each
observation belonging to the second sample as estimated from the
predictor variables.</p>
<p>We first contrast the means <span class="math inline">\(E(y\,|\,\mathbf{x})\)</span> of the different
samples as a function of <span class="math inline">\(\mathbf{x}\)</span>. The command</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="examples.html#cb62-1" aria-hidden="true"></a><span class="kw">c</span>(<span class="kw">mean</span>(yco[zco <span class="op">&lt;</span><span class="st"> </span><span class="dv">0</span>]), <span class="kw">mean</span>(yco[zco <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>]))</span></code></pre></div>
<pre><code>## [1] 23.56415 22.98108</code></pre>
<p>displays the global means of the two samples with mean difference
<span class="math inline">\(0.058\)</span>.</p>
<p>We first create learning and test data sets</p>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="examples.html#cb64-1" aria-hidden="true"></a>dl =<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">6000</span>  <span class="co"># learning`</span></span>
<span id="cb64-2"><a href="examples.html#cb64-2" aria-hidden="true"></a>dt =<span class="st"> </span><span class="dv">6001</span><span class="op">:</span><span class="dv">9357</span> <span class="co"># test`</span></span></code></pre></div>
<p>and contrast the means as a function of <span class="math inline">\(\mathbf{x}\)</span> as follows.</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="examples.html#cb65-1" aria-hidden="true"></a>tree =<span class="st"> </span><span class="kw">contrast</span>(xco[dl, ], yco[dl], zco[dl], <span class="dt">mode =</span> <span class="st">&#39;twosamp&#39;</span>, <span class="dt">type =</span> <span class="st">&#39;diffmean&#39;</span>)</span>
<span id="cb65-2"><a href="examples.html#cb65-2" aria-hidden="true"></a><span class="kw">nodesum</span>(tree, xco[dt,], yco[dt], zco[dt])</span></code></pre></div>
<pre><code>## $nodes
##  [1] 11 21  9 17  2 24 31 33 10 32
## 
## $cri
##  [1] 11.567835  6.843162  5.834586  4.559732  3.305389  3.269056  2.079791
##  [8]  2.008699  1.224605  1.036612
## 
## $wt
##  [1] 307 292 250 287 334 309 275 314 349 640
## 
## $avecri
## [1] 3.790423</code></pre>
<p>Here cri represents the mean difference in each of the nine regions
uncovered by the contrast tree. One sees that there are local regions of
the <span class="math inline">\(\mathbf{x}\)</span> - space where the mean difference between the two
samples is much larger than that of the global means.</p>
<p>Since the contrast tree regions are of finite extent mean differences
within each can originate from two sources. One is due to actual
differences in the conditional distributions <span class="math inline">\(p_{y}(y\,|\)</span>
<span class="math inline">\(\,\mathbf{x,\,}z&lt;0)\,\)</span> and <span class="math inline">\(p_{y}(y\,|\,\mathbf{x,\,}z&gt;0)\)</span> in the
region. The other is differences in the marginal <span class="math inline">\(\mathbf{x}\)</span> -
distributions <span class="math inline">\(p_{\mathbf{x}}(\mathbf{x}\,|\,z&lt;0)\)</span> and
<span class="math inline">\(p_{\mathbf{x}}(\mathbf{x}\,|\,z&gt;0)\)</span> over each individual region. Since
interest is usually in the former one can mitigate the influence of the
latter by propensity weighting based on the propensity probability
scores. The command</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="examples.html#cb67-1" aria-hidden="true"></a><span class="kw">hist</span>(pr2, <span class="dt">nclass =</span> <span class="dv">100</span>)</span></code></pre></div>
<div class="figure"><span id="fig:fig14"></span>
<img src="conTree_tutorial_files/figure-html/fig14-1.png" alt="`hist(pr2, nclass = 100)`" width="672" />
<p class="caption">
Figure 3.14: <code>hist(pr2, nclass = 100)</code>
</p>
</div>
<p>displays the distribution of the input propensity probability scores
in Figure <a href="examples.html#fig:fig14">3.14</a>. One sees that there are moderate
differences between the <span class="math inline">\(\mathbf{x}\)</span> - distributions of the two
samples at least globally.</p>
<p>The propensity weights are calculated with the commands</p>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="examples.html#cb68-1" aria-hidden="true"></a>wp =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, <span class="kw">length</span>(zco))</span>
<span id="cb68-2"><a href="examples.html#cb68-2" aria-hidden="true"></a>wp[zco <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>] =<span class="st"> </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span>pr2[zco <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>]</span>
<span id="cb68-3"><a href="examples.html#cb68-3" aria-hidden="true"></a>wp[zco <span class="op">&lt;</span><span class="st"> </span><span class="dv">0</span>] =<span class="st"> </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>pr2[zco <span class="op">&lt;</span><span class="st"> </span><span class="dv">0</span>])</span>
<span id="cb68-4"><a href="examples.html#cb68-4" aria-hidden="true"></a>wp =<span class="st"> </span><span class="kw">length</span>(wp) <span class="op">*</span><span class="st"> </span>wp <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(wp)</span></code></pre></div>
<p>The corresponding (weighted) contrast tree is obtained as below.</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="examples.html#cb69-1" aria-hidden="true"></a>tree =<span class="st"> </span><span class="kw">contrast</span>(xco[dl, ], yco[dl], zco[dl], <span class="dt">w =</span> wp[dl], <span class="dt">mode =</span> <span class="st">&#39;twosamp&#39;</span>, <span class="dt">type =</span> <span class="st">&#39;diffmean&#39;</span>)</span>
<span id="cb69-2"><a href="examples.html#cb69-2" aria-hidden="true"></a><span class="kw">nodesum</span>(tree, xco[dt, ], yco[dt], zco[dt], <span class="dt">w =</span> wp[dt])</span></code></pre></div>
<pre><code>## $nodes
##  [1] 11 25 17  9 21 28 10 34  2 35
## 
## $cri
##  [1] 8.43006534 6.82843552 4.06862342 3.77038131 2.98880825 2.15116385
##  [7] 1.18141501 1.15381494 0.70247866 0.04491301
## 
## $wt
##  [1] 322.7211 331.9136 266.6065 293.6267 404.4632 347.7717 381.3395 582.1445
##  [9] 353.6817 296.8669
## 
## $avecri
## [1] 2.937557</code></pre>
<p>The component $wt contains the sum of the weights in each region.
Although discrepancies remain, they are somewhat reduced when accounting
for the differences of the <span class="math inline">\(\mathbf{x}\)</span> - distributions within each
region. The command</p>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb71-1"><a href="examples.html#cb71-1" aria-hidden="true"></a><span class="kw">nodeplots</span>(tree, xco[dt,], yco[dt], zco[dt], <span class="dt">w =</span> wp[dt])</span></code></pre></div>
<div class="figure"><span id="fig:fig15"></span>
<img src="conTree_tutorial_files/figure-html/fig15-1.png" alt="`nodeplots(tree, xco[dt,], yco[dt], zco[dt], w = wp[dt])`" width="672" />
<p class="caption">
Figure 3.15: <code>nodeplots(tree, xco[dt,], yco[dt], zco[dt], w = wp[dt])</code>
</p>
</div>
<p>produces the graphical summary shown in Figure <a href="examples.html#fig:fig15">3.15</a>. The
red/blue bars on the lower plot represent the fraction of the total
weights of the first/second sample in each region.</p>
<p>A reference (null) distribution for two sample tree statistics under the
hypothesis of no difference between the samples can be obtained by
permutation testing. The analysis is repeatedly performed with randomly
permuted <span class="math inline">\(z\)</span> - values, thereby randomly assigning observations to each
sample. The commands</p>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb72-1"><a href="examples.html#cb72-1" aria-hidden="true"></a>avedisc =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">1000</span>)</span>
<span id="cb72-2"><a href="examples.html#cb72-2" aria-hidden="true"></a><span class="kw">set.seed</span>(<span class="dv">13</span>)</span>
<span id="cb72-3"><a href="examples.html#cb72-3" aria-hidden="true"></a><span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">1000</span>) {</span>
<span id="cb72-4"><a href="examples.html#cb72-4" aria-hidden="true"></a>    zcot =<span class="st"> </span>zco[<span class="kw">sample.int</span>(<span class="kw">length</span>(zco))]</span>
<span id="cb72-5"><a href="examples.html#cb72-5" aria-hidden="true"></a>    tre =<span class="st"> </span><span class="kw">contrast</span>(xco[dl, ], yco[dl], zcot[dl], <span class="dt">mode =</span> <span class="st">&#39;twosamp&#39;</span>, <span class="dt">type =</span> <span class="st">&#39;diffmean&#39;</span>)</span>
<span id="cb72-6"><a href="examples.html#cb72-6" aria-hidden="true"></a>    avedisc[k] =<span class="st"> </span><span class="kw">nodesum</span>(tre, xco[dt,], yco[dt], zcot[dt])<span class="op">$</span>avecri</span>
<span id="cb72-7"><a href="examples.html#cb72-7" aria-hidden="true"></a>}</span>
<span id="cb72-8"><a href="examples.html#cb72-8" aria-hidden="true"></a><span class="kw">hist</span>(avedisc, <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">4</span>))</span>
<span id="cb72-9"><a href="examples.html#cb72-9" aria-hidden="true"></a><span class="kw">points</span>(<span class="kw">c</span>(<span class="fl">2.937557</span>, <span class="fl">3.790423</span>), <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>), <span class="dt">col =</span> <span class="kw">c</span>(<span class="st">&#39;blue&#39;</span>, <span class="st">&#39;red&#39;</span>))</span></code></pre></div>
<div class="figure"><span id="fig:fig16"></span>
<img src="conTree_tutorial_files/figure-html/fig16-1.png" alt="Two sample null distribution." width="672" />
<p class="caption">
Figure 3.16: Two sample null distribution.
</p>
</div>
<p>compute and display average tree discrepancy for 1000 replications of
this procedure. Figure <a href="examples.html#fig:fig16">3.16</a> shows the histogram of these null discrepancies along
with the corresponding propensity weighted/unweighted alternatives shown
as the blue/red points. One sees that the results based on the original
sample assignments are highly significant.</p>
<p>The command</p>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb73-1"><a href="examples.html#cb73-1" aria-hidden="true"></a><span class="kw">treesum</span>(tree,<span class="kw">c</span>(<span class="dv">11</span>,<span class="dv">25</span>))</span></code></pre></div>
<pre><code>## node 11  var     dir    split 
##           12      +      106 
##           13      +      5232 
##           2      +      1112 
## node 25  var     dir    split 
##           12      +      106 
##           13      -      5232 
##           13      -      4600 
##           12      -      596 
##           6      -      334 
##           5      +      1036</code></pre>
<p>produces the command line output.</p>
<p>As described in Section <a href="contree.html#treesum">2.4</a> the output for each terminal node shows the
sequence of splits that produced its corresponding region. The names
corresponding to each predictor variable can be obtained with the
command</p>
<div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb75-1"><a href="examples.html#cb75-1" aria-hidden="true"></a><span class="kw">names</span>(xco)</span></code></pre></div>
<pre><code>##  [1] &quot;Time&quot;          &quot;PT08.S1.CO.&quot;   &quot;NMHC.GT.&quot;      &quot;C6H6.GT.&quot;     
##  [5] &quot;PT08.S2.NMHC.&quot; &quot;NOx.GT.&quot;       &quot;PT08.S3.NOx.&quot;  &quot;NO2.GT.&quot;      
##  [9] &quot;PT08.S4.NO2.&quot;  &quot;PT08.S5.O3.&quot;   &quot;T&quot;             &quot;RH&quot;           
## [13] &quot;AH&quot;</code></pre>
<p>so that the boundaries of nodes 11 and 25 are defined by</p>
<table>
<thead>
<tr class="header">
<th align="left">Node</th>
<th align="left">Rule</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><code>11</code></td>
<td align="left"><code>RH&gt;106 &amp; AH&gt;5232 &amp; NMHC.GT &gt;1112</code></td>
</tr>
<tr class="even">
<td align="left"><code>25</code></td>
<td align="left"><code>106&lt;=RH&lt; 596 &amp; AH&lt;=4600 &amp; NOx.GT&lt;334</code></td>
</tr>
</tbody>
</table>
<p>We next contrast the conditional distributions <span class="math inline">\(y\,|\,\mathbf{x}\)</span> of
the two samples. Figure <a href="examples.html#fig:fig17">3.17</a> shows a QQ-plot between the
respective <span class="math inline">\(y\)</span> global distributions on the test data obtained by the
commands</p>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb77-1"><a href="examples.html#cb77-1" aria-hidden="true"></a>ycodt =<span class="st"> </span>yco[dt]</span>
<span id="cb77-2"><a href="examples.html#cb77-2" aria-hidden="true"></a><span class="kw">qqplot</span>(ycodt[zco[dt] <span class="op">&lt;</span><span class="st"> </span><span class="dv">0</span>], ycodt[zco[dt] <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>], <span class="dt">pch =</span> <span class="st">&#39;.&#39;</span>)</span>
<span id="cb77-3"><a href="examples.html#cb77-3" aria-hidden="true"></a><span class="kw">lines</span>(<span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">200</span>), <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">200</span>), <span class="dt">col =</span> <span class="st">&#39;red&#39;</span>)</span></code></pre></div>
<div class="figure"><span id="fig:fig17"></span>
<img src="conTree_tutorial_files/figure-html/fig17-1.png" alt="`qqplot(ycodt[zco[dt] &lt; 0], ycodt[zco[dt] &gt; 0], pch = '.')`" width="672" />
<p class="caption">
Figure 3.17: <code>qqplot(ycodt[zco[dt] &lt; 0], ycodt[zco[dt] &gt; 0], pch = '.')</code>
</p>
</div>
<p>The global <span class="math inline">\(y\)</span> - distributions of the two samples are seen to be nearly
identical. The corresponding (propensity weighted) contrast tree results
are obtained with the commands</p>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb78-1"><a href="examples.html#cb78-1" aria-hidden="true"></a>tree =<span class="st"> </span><span class="kw">contrast</span>(xco[dl, ], yco[dl], zco[dl], <span class="dt">w =</span> wp[dl], <span class="dt">mode =</span> <span class="st">&#39;twosamp&#39;</span>, <span class="dt">tree.size =</span> <span class="dv">9</span>)</span>
<span id="cb78-2"><a href="examples.html#cb78-2" aria-hidden="true"></a><span class="kw">nodeplots</span>(tree, xco[dt,], yco[dt], zco[dt], <span class="dt">w =</span> wp[dt])</span></code></pre></div>
<p><img src="conTree_tutorial_files/figure-html/unnamed-chunk-45-1.png" width="672" /></p>
<p>as shown in Figure @ref{fig:fig16). The tree has uncovered a few
regions where there are moderate differences between the two
distributions.</p>
</div>
<div id="session-information" class="section level2" number="3.5">
<h2><span class="header-section-number">3.5</span> Session Information</h2>
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb79-1"><a href="examples.html#cb79-1" aria-hidden="true"></a><span class="kw">sessionInfo</span>()</span></code></pre></div>
<pre><code>## R version 4.0.2 (2020-06-22)
## Platform: x86_64-apple-darwin19.5.0 (64-bit)
## Running under: macOS Catalina 10.15.6
## 
## Matrix products: default
## BLAS/LAPACK: /usr/local/Cellar/openblas/0.3.10/lib/libopenblasp-r0.3.10.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices datasets  utils     methods   base     
## 
## other attached packages:
## [1] conTree_0.2-5  rmarkdown_2.3  knitr_1.28     pkgdown_1.5.1  devtools_2.3.0
## [6] usethis_1.6.1 
## 
## loaded via a namespace (and not attached):
##  [1] rstudioapi_0.11   magrittr_1.5      MASS_7.3-51.6     pkgload_1.1.0    
##  [5] R6_2.4.1          rlang_0.4.7       fansi_0.4.1       highr_0.8        
##  [9] stringr_1.4.0     tools_4.0.2       pkgbuild_1.0.8    xfun_0.15        
## [13] sessioninfo_1.1.1 cli_2.0.2         withr_2.2.0       htmltools_0.5.0  
## [17] ellipsis_0.3.1    remotes_2.1.1     yaml_2.2.1        assertthat_0.2.1 
## [21] digest_0.6.25     rprojroot_1.3-2   bookdown_0.19     crayon_1.3.4     
## [25] processx_3.4.2    callr_3.4.3       fs_1.4.2          ps_1.3.3         
## [29] testthat_2.3.2    evaluate_0.14     memoise_1.1.0     glue_1.4.1       
## [33] stringi_1.4.6     compiler_4.0.2    desc_1.2.0        backports_1.1.8  
## [37] prettyunits_1.1.1</code></pre>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-FriedmanPNAS">
<p>Friedman, Jerome. 2020. “Contrast Trees and Distribution Boosting.” <em>PNAS (to Appear)</em>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="contree.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="acknowledgment.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["conTree_tutorial.pdf", "conTree_tutorial.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
